

from typing import *
import numpy as np
from .basegenerator import BaseGenerator


__all__ = ("ExploitExploreGenerator",)


class ExploitExploreGenerator(BaseGenerator):
    """ This generator blends a randomly generated point with the location of an existing optimizer. The optimizer is
        chosen based on a roulette selection.
    """

    def __init__(self, bounds: Sequence[Tuple[float, float]], max_func_calls: int, focus: float = 1,
                 pop_poor_opts: bool = True):
        """

        Parameters
        ----------
        bounds: Sequence[Tuple[float, float]]
            Min and max bounds for each parameter.
        max_func_calls: int
            Maximum function calls allowed for the optimization, at and beyond this point there is a 100% chance that a
            previously evaluated point will be returned by the generator.
        focus: float = 1
            The blend parameter between random point and incumbent is given as p=(f_calls / max_f_calls)^focus. At p=0
            the random point is taken at p=1 the incumbent is chosen. If focus<1 points are more like the incumbent, if
            focus>1 points are more like the random. Default is focus=1 which has a linear growth from random to
            incumbent. The new pt is calculated as new_pt = p*incumbent_pt + (1-p)*random_pt.
        pop_poor_opts: bool = True
            If True, removes all optimizers that are more than three orders of magnitude larger than the best
            optimizer. Roulette selection between optimizers is based on their best evaluation. In cases with large
            order of magnitude differences this can distort the selection such that all optimizers except the worst have
            nearly equal probability.
        """
        super().__init__()
        self.max_func_calls = max_func_calls
        self.focus = focus
        self.pop_poor_opts = pop_poor_opts

        bnds = np.array(bounds)
        self.min = bnds[:, 0]
        self.max = bnds[:, 1]
        self.n_params = len(bnds)

    def generate(self, manager: 'GloMPOManager') -> np.ndarray:
        x_track = []
        f_track = []

        # Random Point
        random = (self.max - self.min) * np.random.random(self.n_params) + self.min
        self.logger.debug(f"Random = {random}")

        for o in range(1, manager.o_counter):
            history = manager.opt_log.get_history(o)
            if history:
                i_max = len(history)
                i_best = history[i_max][3]
                f_best = history[i_max][2]
                x_best = history[i_best][5]

                x_track.append(x_best)
                f_track.append(f_best)

        if len(f_track) != 0 and self.pop_poor_opts:
            self.logger.debug(f"f_track before purge = {f_track}")
            bool_track = [f+min(f_track) < 1000 for f in f_track]
            f_track = np.array(f_track)[bool_track]
            self.logger.debug(f"f_track after purge = {f_track}")
            x_track = np.array(x_track)[bool_track]
        else:
            x_track = np.array([])
            f_track = np.array([])

        assert len(f_track) == len(x_track)

        if len(f_track) == 0:
            return random

        # Roulette Selection
        f_track = np.append(f_track, 1.1*np.amax(f_track))

        shift = f_track - np.min(f_track)
        revert = np.max(shift) - shift
        if np.all(revert == 0):
            return random
        prob = revert / np.sum(revert)

        select = np.random.choice(range(len(f_track)), p=prob)
        incumbent = x_track[select]
        self.logger.debug(f"Selected incumbent from Optimizer {select} = {incumbent}")

        # Blending parameter
        f_calls = np.clip(manager.f_counter / self.max_func_calls, 0, 1)
        alpha = f_calls ** self.focus
        self.logger.debug(f"Selected alpha = {alpha}")

        # New point
        generated = alpha*incumbent + (1-alpha)*random
        self.logger.debug(f"Generated = {generated}")

        return generated
